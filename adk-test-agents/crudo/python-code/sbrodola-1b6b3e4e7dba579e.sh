#!/bin/bash

# sbrodola.sh - Generates the Streamlit Cloud Run Investigator structure

echo "üöÄ Creating directories..."
mkdir -p lib/streamlit
mkdir -p .cache # Just in case it doesn't exist

echo "üé® Creating your favorite color library placeholder (if needed)..."
# Assuming your existing lib/ricc_colors.py is sufficient.
# If you needed a new one generated:
# cat << EOF > lib/ricc_colors.py
# # Your awesome color definitions go here!
# # Example:
# COLOR_RED = "\033[91m"
# COLOR_GREEN = "\033[92m"
# COLOR_YELLOW = "\033[93m"
# COLOR_BLUE = "\033[94m"
# COLOR_MAGENTA = "\033[95m"
# COLOR_CYAN = "\033[96m"
# COLOR_BOLD = "\033[1m"
# COLOR_UNDERLINE = "\033[4m"
# COLOR_END = "\033[0m"

# def red(text): return f"{COLOR_RED}{text}{COLOR_END}"
# # ... add functions for other colors and styles
# EOF

echo "üêç Creating Python files..."

# --- app.py ---
cat << EOF > app.py
import streamlit as st
import os
from pathlib import Path

# Assuming your existing libs are importable
from lib.streamlit import state, ui, database, helpers
# Import specific functions from existing libs if needed directly here
# e.g., from lib.ricc_gcp_projects import get_projects_by_user
# However, it's better to call them from ui.py or state.py

# --- App Configuration ---
APP_TITLE = "Cloud Run Investigator üïµÔ∏è‚Äç‚ôÇÔ∏è‚òÅÔ∏è"
DB_FILE = Path("convos.sqlite3")
CACHE_DIR = Path(".cache") # Base cache directory

st.set_page_config(page_title=APP_TITLE, layout="wide")
st.title(APP_TITLE)

# --- Initialization ---
# Ensure DB exists
database.initialize_database(DB_FILE)

# Initialize session state (config, investigations, etc.)
state.initialize_session_state(DB_FILE)

# --- Load Data ---
# Load investigations from DB into session state if not already loaded
if "investigations_loaded" not in st.session_state:
    state.load_investigations_from_db()
    st.session_state.investigations_loaded = True # Mark as loaded

# --- UI Rendering ---
# Render the sidebar (controls, investigation list)
# Pass necessary functions or data from existing libs if needed
# e.g. ui.render_sidebar(get_projects_func=get_projects_by_user)
ui.render_sidebar(cache_dir=CACHE_DIR)

# Render the main chat area
# Pass the Gemini interaction function
# e.g. ui.render_chat_interface(gemini_chat_func=your_gemini_function)
ui.render_chat_interface(db_file=DB_FILE)

# Optional: Add a footer or debug info
# st.sidebar.markdown("---")
# st.sidebar.json(st.session_state)

EOF

# --- lib/__init__.py ---
# Ensure lib is treated as a package
touch lib/__init__.py

# --- lib/streamlit/__init__.py ---
touch lib/streamlit/__init__.py

# --- lib/streamlit/state.py ---
cat << EOF > lib/streamlit/state.py
import streamlit as st
import os
import uuid
from datetime import datetime

# Assuming your existing libs are importable
from lib.streamlit import database
# Import necessary functions from your core libs
# from lib.ricc_utils import get_debug_mode # Example

DEFAULT_USER_EMAIL_PATTERN = "{user}@google.com"
DEFAULT_FALLBACK_EMAIL = "ricc@google.com"
DEFAULT_GEMINI_MODEL = "gemini-1.5-flash"
DEFAULT_REGION = "europe-west1"

def get_default_user_id():
    """Gets user ID from ENV or defaults."""
    try:
        user = os.environ.get("USER")
        if user:
            return DEFAULT_USER_EMAIL_PATTERN.format(user=user)
        else:
            st.warning("‚ö†Ô∏è Could not determine username from ENV[USER]. Falling back.")
            return DEFAULT_FALLBACK_EMAIL
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Error getting username: {e}. Falling back.")
        return DEFAULT_FALLBACK_EMAIL

def get_default_gemini_model():
    """Gets Gemini model from ENV or defaults."""
    return os.environ.get("GEMINI_MODEL", DEFAULT_GEMINI_MODEL)

def get_default_cloud_region():
    """Gets Cloud Region from ENV or defaults."""
    return os.environ.get("GOOGLE_CLOUD_LOCATION", DEFAULT_REGION)

def initialize_session_state(db_file):
    """Initializes Streamlit's session state variables."""
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
        st.session_state.debug_mode = os.environ.get("DEBUG", "false").lower() == "true"
        # Debug mode might come from your ricc_utils or similar
        # st.session_state.debug_mode = get_debug_mode()

        # Configuration
        st.session_state.user_id = get_default_user_id()
        st.session_state.gemini_model = get_default_gemini_model()
        st.session_state.cloud_region = get_default_cloud_region()

        # GCP Selection State
        st.session_state.project_id = None
        st.session_state.service_name = None
        st.session_state.available_projects = [] # To cache project list
        st.session_state.available_services = [] # To cache service list

        # Investigation / Chat State
        st.session_state.investigations = {} # Dict mapping investigation_id -> {name: str, history: list[dict]}
        st.session_state.current_investigation_id = None
        st.session_state.db_file = db_file # Store DB file path

        # Add a default "Welcome" investigation if none exist after loading
        if not st.session_state.investigations:
             create_new_investigation(name="üëã Welcome Investigation", activate=True)

        print("‚ú® Session state initialized.")

def create_new_investigation(name=None, context=None, activate=True):
    """Creates a new investigation entry."""
    investigation_id = str(uuid.uuid4())
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    if not name:
        name = f"Investigation {timestamp}"

    initial_message = {
        "role": "assistant",
        "parts": [{"text": f"üïµÔ∏è‚Äç‚ôÇÔ∏è Starting new investigation: {name}" + (f"\nContext: {context}" if context else "")}]
    }

    new_investigation = {
        "name": name,
        "created_at": timestamp,
        "history": [initial_message],
        "project_id": st.session_state.get('project_id'), # Capture context
        "service_name": st.session_state.get('service_name'),
        "region": st.session_state.get('cloud_region')
    }
    st.session_state.investigations[investigation_id] = new_investigation

    # Persist immediately
    database.save_investigation(st.session_state.db_file, investigation_id, new_investigation)

    if activate:
        st.session_state.current_investigation_id = investigation_id
        print(f"üöÄ Created and activated new investigation: {investigation_id} - {name}")
    else:
        print(f"üöÄ Created new investigation: {investigation_id} - {name}")

    st.rerun() # Rerun to update UI immediately

def switch_investigation(investigation_id):
    """Switches the active investigation."""
    if investigation_id in st.session_state.investigations:
        st.session_state.current_investigation_id = investigation_id
        print(f"üîÑ Switched to investigation: {investigation_id}")
        st.rerun()
    else:
        st.error(f"üö´ Investigation ID {investigation_id} not found!")

def load_investigations_from_db():
    """Loads all investigations from the database into session state."""
    investigations_data = database.load_all_investigations(st.session_state.db_file)
    st.session_state.investigations = investigations_data
    if not st.session_state.current_investigation_id and investigations_data:
        # Activate the most recent one if none is active
        st.session_state.current_investigation_id = list(investigations_data.keys())[0] # Or sort by timestamp
    elif not investigations_data:
        # If DB is empty, create the welcome one
         create_new_investigation(name="üëã Welcome Investigation", activate=True)

    print(f"üìö Loaded {len(investigations_data)} investigations from DB.")

def add_message_to_current_investigation(role: str, text: str = None, parts: list = None, chart_filename: str = None):
    """Adds a message to the history of the current investigation and saves."""
    if not st.session_state.current_investigation_id:
        st.error("üö® No active investigation to add message to!")
        return

    current_id = st.session_state.current_investigation_id
    investigation = st.session_state.investigations[current_id]

    message_content = {}
    if text:
        message_content["text"] = text
    if parts:
         message_content["parts"] = parts # Gemini format

    message = {"role": role, "content": message_content} # Adapt based on Gemini structure

    # Add chart info if provided
    if chart_filename:
        message["chart_filename"] = str(chart_filename) # Store as string

    investigation["history"].append(message)

    # Update the investigation in session state
    st.session_state.investigations[current_id] = investigation

    # Save the updated investigation to DB
    database.save_investigation(st.session_state.db_file, current_id, investigation)
    print(f"üí¨ Added message to investigation {current_id} and saved.")

EOF

# --- lib/streamlit/ui.py ---
cat << EOF > lib/streamlit/ui.py
import streamlit as st
from pathlib import Path
import time # For simulating streaming

# Assuming your existing libs are importable
from lib.streamlit import state, helpers, database # May need database here or pass db_file
# Import specific functions needed for UI (selectors, etc.)
from lib.ricc_gcp import get_projects_by_user # Placeholder - Ensure this exists!
from lib.ricc_cloud_run import get_cloud_run_endpoints # Placeholder - Ensure this exists!
from lib.ricc_genai import generate_content_with_function_calling # Placeholder!

# --- Constants ---
USER_AVATAR = "üßë‚Äçüíª"
MODEL_AVATAR = "ü§ñ" # Or "‚ú®" for Gemini?

# --- Sidebar Rendering ---
def render_sidebar(cache_dir: Path):
    """Renders the sidebar content."""
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        st.info(f"User: {st.session_state.user_id}")
        # Allow changing model/region? Maybe later.
        st.caption(f"Model: {st.session_state.gemini_model}")
        st.caption(f"Region: {st.session_state.cloud_region}")

        st.markdown("---")

        # --- Project & Service Selection ---
        st.header("üéØ Target Service")

        # Project Selector
        with st.spinner("Fetching projects..."):
             # Cache the project list fetch if it's slow
             if not st.session_state.available_projects:
                  try:
                       st.session_state.available_projects = get_projects_by_user(st.session_state.user_id)
                  except Exception as e:
                       st.error(f"Failed to get projects: {e}")
                       st.session_state.available_projects = []

        selected_project = st.selectbox(
            "Select GCP Project:",
            options=[p['projectId'] for p in st.session_state.available_projects], # Assuming list of dicts
            index=None, # Default to none selected
            key="project_id", # Binds to session state automatically
            placeholder="Choose a project...",
        )

        # Service Selector (only if project is selected)
        selected_service = None
        if selected_project:
             # Fetch services for the selected project and region
             # Add caching here too if needed
             with st.spinner(f"Fetching Cloud Run services for {selected_project}..."):
                  try:
                      # Assuming get_cloud_run_endpoints returns a list of service names (strings)
                      st.session_state.available_services = get_cloud_run_endpoints(
                          project_id=selected_project,
                          region=st.session_state.cloud_region
                      )
                  except Exception as e:
                      st.error(f"Failed to get services for {selected_project}: {e}")
                      st.session_state.available_services = []

             selected_service = st.selectbox(
                  f"Select Cloud Run Service (in {st.session_state.cloud_region}):",
                  options=st.session_state.available_services,
                  index=None,
                  key="service_name", # Binds to session state
                  placeholder="Choose a service..."
             )

        # Display Service Info (if service selected)
        if selected_project and selected_service:
             st.markdown("---")
             st.subheader(f"üìå Service Info: {selected_service}")
             with st.spinner("Loading service details..."):
                 service_info, service_urls, monitor_charts = helpers.load_service_data_from_cache(
                     cache_dir, selected_project, selected_service
                 )

                 if service_urls:
                      st.write("**URLs:**")
                      for url in service_urls:
                           st.code(url, language=None) # Display URLs clearly
                 else:
                     st.caption("No public URLs found in service.yaml")

                 if service_info:
                      with st.expander("Service Configuration (service.yaml)"):
                           st.text(service_info) # Or st.code(service_info, language='yaml') if you have pyyaml
                 else:
                      st.caption("service.yaml not found in cache.")

                 if monitor_charts:
                      st.write("**Monitoring Charts:**")
                      cols = st.columns(3) # Adjust grid as needed
                      col_idx = 0
                      for chart_path in monitor_charts:
                           with cols[col_idx % len(cols)]:
                                try:
                                    st.image(str(chart_path), caption=chart_path.name, use_column_width=True)
                                except Exception as e:
                                    st.error(f"Error loading {chart_path.name}: {e}")
                           col_idx += 1
                 else:
                     st.caption("No monitoring charts found in cache.")

             # Button to start investigation for this service
             if st.button(f"üïµÔ∏è Start Investigation for {selected_service}", use_container_width=True):
                 context = f"Project: {selected_project}, Service: {selected_service}, Region: {st.session_state.cloud_region}"
                 state.create_new_investigation(
                     name=f"Investigate {selected_service}",
                     context=context,
                     activate=True
                 )
                 # State change triggers rerun

        st.markdown("---")

        # --- Investigation Management ---
        st.header("üìú Investigations")

        if st.button("‚ûï New Investigation", use_container_width=True):
             state.create_new_investigation(activate=True)
             # state handles rerun

        # List existing investigations
        if st.session_state.investigations:
            sorted_investigations = sorted(
                st.session_state.investigations.items(),
                key=lambda item: item[1].get('created_at', '0'), # Sort by creation time
                reverse=True
            )
            for inv_id, inv_data in sorted_investigations:
                button_label = f"{inv_data.get('name', 'Untitled Investigation')}"
                button_type = "primary" if inv_id == st.session_state.current_investigation_id else "secondary"
                if st.button(button_label, key=f"inv_{inv_id}", type=button_type, use_container_width=True):
                    state.switch_investigation(inv_id)
                    # state handles rerun
        else:
            st.caption("No investigations yet. Start one!")


# --- Chat Interface Rendering ---
def render_chat_interface(db_file):
    """Renders the main chat area and handles user input."""

    if not st.session_state.current_investigation_id:
        st.info("üëà Select an investigation or start a new one from the sidebar.")
        return

    current_id = st.session_state.current_investigation_id
    investigation = st.session_state.investigations.get(current_id)

    if not investigation:
        st.error(f"üö® Could not load investigation {current_id}. Try selecting another.")
        st.session_state.current_investigation_id = None # Reset
        time.sleep(1) # Brief pause before rerun
        st.rerun()
        return

    st.subheader(f"üí¨ Chat: {investigation.get('name', 'Current Investigation')}")

    # Display chat messages
    for message in investigation["history"]:
        role = message.get("role")
        avatar = USER_AVATAR if role == "user" else MODEL_AVATAR
        with st.chat_message(role, avatar=avatar):
            # Display text content (handle potential structures)
            content = message.get("content", {}) # Assuming content is a dict
            text_parts = content.get("parts", []) if isinstance(content,dict) else [{"text": str(content)}] # Handle simple text too
            full_text = "\n".join([part.get("text", "") for part in text_parts if "text" in part])
            if full_text:
                st.markdown(full_text) # Render markdown

            # Display charts if available
            chart_filename = message.get("chart_filename")
            if chart_filename:
                chart_path = Path(chart_filename)
                if chart_path.is_file():
                    try:
                        st.image(str(chart_path), caption=f"üìä Chart: {chart_path.name}")
                    except Exception as e:
                        st.error(f"Failed to display chart {chart_path.name}: {e}")
                else:
                    st.warning(f"Chart file not found: {chart_filename}")

    # Chat input
    if prompt := st.chat_input("Ask Gemini about the Cloud Run service..."):
        # 1. Add user message to state and display it immediately
        state.add_message_to_current_investigation(role="user", text=prompt) # Persists here
        with st.chat_message("user", avatar=USER_AVATAR):
            st.markdown(prompt)

        # 2. Prepare history for Gemini
        # Adapt this based on how your ricc_genai expects history
        gemini_history = investigation["history"][:-1] # Send history *before* user prompt? Or include it? Check API.

        # 3. Call Gemini (handle streaming and function calling)
        try:
            with st.chat_message("assistant", avatar=MODEL_AVATAR):
                message_placeholder = st.empty()
                full_response_text = ""
                final_chart_filename = None

                # --- Streaming Call ---
                # Replace with your actual function call
                # This needs to handle function calls internally if possible, or return info about them
                # For now, simulate streaming:
                # response_stream = generate_content_with_function_calling(
                #     model_name=st.session_state.gemini_model,
                #     prompt=prompt,
                #     history=gemini_history,
                #     # ... other params like tools=...
                # )

                # Simulated stream with potential chart info at the end
                simulated_stream = ["Thinking... ü§î\n\n", "Okay, ", "let's look ", "into that. ", "Checking the ", "service status... ", "\n\nFound something! ", "Generating a chart... üìä"]
                simulated_chart = "path/to/your/generated/chart.png" # Replace with actual logic result

                # Use st.write_stream for real Gemini stream
                # full_response_text = message_placeholder.write_stream(response_stream)

                # Simulation logic:
                for chunk in simulated_stream:
                    full_response_text += chunk
                    message_placeholder.markdown(full_response_text + "‚ñå") # Simulate cursor
                    time.sleep(0.1)
                message_placeholder.markdown(full_response_text) # Final text without cursor
                final_chart_filename = simulated_chart # Set chart filename after text stream

                # --- Process Function Calls (if needed after streaming) ---
                # If your generate_content function returns function call info separately:
                # function_call = response.get("function_call") # Example structure
                # if function_call:
                #    tool_response = execute_function(function_call) # Your logic
                #    # Send tool_response back to Gemini in another call...
                #    # ... update message_placeholder with final Gemini answer
                #    pass # Placeholder

                # Display chart if generated
                if final_chart_filename:
                    chart_path = Path(final_chart_filename)
                    if chart_path.is_file():
                         try:
                              st.image(str(chart_path), caption=f"üìä Generated Chart: {chart_path.name}")
                         except Exception as e:
                              st.error(f"Failed display generated chart {chart_path.name}: {e}")
                    else:
                         st.warning(f"Generated chart file not found: {final_chart_filename}")


            # 4. Add final assistant response (including chart info) to state
            state.add_message_to_current_investigation(
                 role="assistant",
                 text=full_response_text, # Or use 'parts' if Gemini response uses it
                 chart_filename=final_chart_filename
            )
            # Note: Rerun is not needed here unless adding the message itself failed

        except Exception as e:
            st.error(f"ü§ñ Oops! Error calling Gemini: {e}")
            # Optionally add error message to chat history
            # state.add_message_to_current_investigation(role="assistant", text=f"Error: {e}")

EOF


# --- lib/streamlit/database.py ---
cat << EOF > lib/streamlit/database.py
import sqlite3
import json
from pathlib import Path
import streamlit as st # For error reporting potentially
from contextlib import closing

def initialize_database(db_file: Path):
    """Creates the database and table if they don't exist."""
    if db_file.exists():
        print(f"Database {db_file} already exists.")
        # You might want to add schema migration logic here in the future
    else:
        print(f"Creating database {db_file}...")
        try:
            with closing(sqlite3.connect(db_file)) as conn:
                with closing(conn.cursor()) as cursor:
                    cursor.execute("""
                        CREATE TABLE investigations (
                            id TEXT PRIMARY KEY,
                            name TEXT,
                            created_at TEXT,
                            project_id TEXT,
                            service_name TEXT,
                            region TEXT,
                            history TEXT -- Store history as JSON string
                        )
                    """)
                    conn.commit()
            print("Database and table created successfully.")
        except sqlite3.Error as e:
            st.error(f"üö® Database Error during initialization: {e}")
            raise # Propagate error if init fails

def save_investigation(db_file: Path, investigation_id: str, investigation_data: dict):
    """Saves or updates a single investigation in the database."""
    try:
        with closing(sqlite3.connect(db_file)) as conn:
            with closing(conn.cursor()) as cursor:
                # Serialize history to JSON string
                history_json = json.dumps(investigation_data.get("history", []))

                cursor.execute("""
                    INSERT INTO investigations (id, name, created_at, project_id, service_name, region, history)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                    ON CONFLICT(id) DO UPDATE SET
                        name=excluded.name,
                        created_at=excluded.created_at,
                        project_id=excluded.project_id,
                        service_name=excluded.service_name,
                        region=excluded.region,
                        history=excluded.history
                """, (
                    investigation_id,
                    investigation_data.get("name"),
                    investigation_data.get("created_at"),
                    investigation_data.get("project_id"),
                    investigation_data.get("service_name"),
                    investigation_data.get("region"),
                    history_json
                ))
                conn.commit()
        # print(f"Saved investigation {investigation_id} to DB.") # Can be verbose
    except sqlite3.Error as e:
        st.error(f"üö® Database Error saving investigation {investigation_id}: {e}")
    except json.JSONDecodeError as e:
        st.error(f"üö® JSON Error serializing history for investigation {investigation_id}: {e}")


def load_all_investigations(db_file: Path) -> dict:
    """Loads all investigations from the database."""
    investigations = {}
    try:
        with closing(sqlite3.connect(db_file)) as conn:
            # conn.row_factory = sqlite3.Row # Makes accessing columns easier
            with closing(conn.cursor()) as cursor:
                cursor.execute("SELECT id, name, created_at, project_id, service_name, region, history FROM investigations ORDER BY created_at DESC") # Load newest first
                rows = cursor.fetchall()
                for row in rows:
                    inv_id, name, created_at, project_id, service_name, region, history_json = row
                    try:
                        history = json.loads(history_json) if history_json else []
                        investigations[inv_id] = {
                            "name": name,
                            "created_at": created_at,
                            "project_id": project_id,
                            "service_name": service_name,
                            "region": region,
                            "history": history
                        }
                    except json.JSONDecodeError as e:
                        st.warning(f"‚ö†Ô∏è Could not parse history for investigation {inv_id}: {e}. Skipping history.")
                        # Still load the investigation entry, but with empty history
                        investigations[inv_id] = {
                            "name": name, "created_at": created_at, "project_id": project_id,
                            "service_name": service_name, "region": region, "history": []
                        }
    except sqlite3.Error as e:
        st.error(f"üö® Database Error loading investigations: {e}")
        # Return empty dict on load failure? Or raise?
    return investigations

# Add functions like delete_investigation if needed later

EOF

# --- lib/streamlit/helpers.py ---
cat << EOF > lib/streamlit/helpers.py
import streamlit as st
from pathlib import Path
import yaml # Requires PyYAML: pip install pyyaml

# Define expected cache structure relative to project/service
SERVICE_YAML_NAME = "service.yaml"
MONITORING_CHARTS_DIR = "mon-charts"

@st.cache_data(ttl=300) # Cache for 5 minutes to avoid constant reading
def load_service_data_from_cache(cache_dir: Path, project_id: str, service_name: str):
    """
    Loads service.yaml content, extracts URLs, and finds monitoring chart images
    from the predefined cache structure.

    Args:
        cache_dir: Base cache directory (e.g., Path(".cache")).
        project_id: The GCP project ID.
        service_name: The Cloud Run service name.

    Returns:
        Tuple: (service_yaml_content, list_of_urls, list_of_chart_paths)
               Returns (None, [], []) if data is not found.
    """
    service_cache_dir = cache_dir / project_id / "cloud-run" / service_name
    service_yaml_path = service_cache_dir / SERVICE_YAML_NAME
    charts_dir_path = service_cache_dir / MONITORING_CHARTS_DIR

    service_yaml_content = None
    urls = []
    chart_paths = []

    # 1. Load service.yaml and extract URLs
    if service_yaml_path.is_file():
        try:
            with open(service_yaml_path, 'r') as f:
                service_yaml_content = f.read() # Return raw content for display
            # Attempt to parse YAML to extract URLs safely
            try:
                service_data = yaml.safe_load(service_yaml_content)
                # Navigate safely: check keys exist before accessing
                status = service_data.get('status', {})
                traffic_list = status.get('traffic', [])
                for traffic_item in traffic_list:
                    url = traffic_item.get('url')
                    if url:
                        urls.append(url)
                # Add other URL sources if needed (e.g., status.url)
                main_url = status.get('url')
                if main_url and main_url not in urls:
                     urls.insert(0, main_url) # Put primary URL first

            except yaml.YAMLError as e:
                st.warning(f"‚ö†Ô∏è Could not parse {service_yaml_path} to extract URLs: {e}")
            except AttributeError as e:
                 st.warning(f"‚ö†Ô∏è Unexpected structure in {service_yaml_path} when looking for URLs: {e}")

        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error reading {service_yaml_path}: {e}")
            service_yaml_content = f"Error reading file: {e}" # Show error in UI

    # 2. Find monitoring charts (common image formats)
    if charts_dir_path.is_dir():
        for item in charts_dir_path.glob('*'):
            if item.is_file() and item.suffix.lower() in ['.png', '.jpg', '.jpeg', '.gif', '.webp']:
                chart_paths.append(item)
        chart_paths.sort() # Sort alphabetically

    return service_yaml_content, urls, chart_paths

# Add any other Streamlit-specific helper functions here

EOF

echo "‚úÖ Done creating files."
echo "üëâ Next steps:"
echo "1. Ensure all dependencies are installed (streamlit, google-cloud-aiplatform, google-cloud-run, google-cloud-resource-manager, pyyaml)."
echo "   pip install streamlit google-cloud-aiplatform google-cloud-run google-cloud-resource-manager pyyaml"
echo "2. Review the generated files, especially the TODOs and placeholders."
echo "3. Implement the actual calls to your existing libraries (e.g., get_projects_by_user, get_cloud_run_endpoints, generate_content_with_function_calling)."
echo "4. Implement the chart generation logic if Gemini function calling returns instructions to do so."
echo "5. Run the app: streamlit run app.py"
