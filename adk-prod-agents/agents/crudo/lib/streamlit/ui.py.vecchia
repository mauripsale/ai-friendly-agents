import streamlit as st
from pathlib import Path
import time # For simulating streaming

# Assuming your existing libs are importable
from lib.streamlit import state, helpers, database # May need database here or pass db_file
# Import specific functions needed for UI (selectors, etc.)
from lib.ricc_gcp_projects import get_projects_by_user # Placeholder - Ensure this exists!
from lib.ricc_cloud_run import get_cloud_run_endpoints # Placeholder - Ensure this exists!
from lib.ricc_genai import generate_content_with_function_calling # Placeholder!
from lib.ricc_utils import get_projects_by_user_faker

# --- Constants ---
USER_AVATAR = "üßë‚Äçüíª"
MODEL_AVATAR = "ü§ñ" # Or "‚ú®" for Gemini?

# --- Sidebar Rendering ---
def render_sidebar(cache_dir: Path):
    """Renders the sidebar content."""
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        st.info(f"User: {st.session_state.user_id}")
        # Allow changing model/region? Maybe later.
        st.caption(f"Model: {st.session_state.gemini_model}")
        st.caption(f"Region: {st.session_state.cloud_region}")

        st.markdown("---")

        # --- Project & Service Selection ---
        st.header("üéØ Target Service")

        # Project Selector
        with st.spinner("Fetching projects..."):
             # Cache the project list fetch if it's slow
             if not st.session_state.available_projects:
                  try:
                       #st.session_state.available_projects = get_projects_by_user(st.session_state.user_id)
                       st.session_state.available_projects = get_projects_by_user_faker(st.session_state.user_id)
                  except Exception as e:
                       st.error(f"Failed to get projects: {e}")
                       st.session_state.available_projects = []

        selected_project = st.selectbox(
            "Select GCP Project:",
            #options=[p['projectId'] for p in st.session_state.available_projects], # Assuming list of dicts
            options=st.session_state.available_projects, # Assuming it's already a list of project ID strings
            #options=[p['projectId'] for p in st.session_state.available_projects], # <--- ERROR HERE
            index=None, # Default to none selected
            key="project_id", # Binds to session state automatically
            placeholder="Choose a project...",
        )

        # Service Selector (only if project is selected)
        selected_service = None
        if selected_project:
             # Fetch # The above code is not valid Python code. It seems to be a mix of comments and the word "services".
             # If you have a specific question or need help with Python code, please provide more information.
             # services for the selected project and region
             # Add caching here too if needed
             with st.spinner(f"Fetching Cloud Run services for {selected_project}..."):
                  try:
                      # Assuming get_cloud_run_endpoints returns a list of service names (strings)
                      endpoints_answer = get_cloud_run_endpoints(
                          project_id=selected_project,
                          region=st.session_state.cloud_region
                      )
                      print(f"DEB endpoints_answer: {endpoints_answer}")
                      if endpoints_answer['status'] in ['success_api', 'success_cache']:
                          pass # All good
                      else:
                          raise Exception(f"Exception in get_cloud_run_endpoints(): status not ok: '{endpoints_answer['status']}'")
                      service_names = [ srv_dict['name'] for srv_dict in endpoints_answer['services'] ]
                      if service_names == []:
                          service_names = [ "üòû Empty! (Change project please)"]
                      st.session_state.available_services = sorted(service_names)
                  except Exception as e:
                      st.error(f"Failed to get services for {selected_project}: {e}")
                      st.session_state.available_services = []

             selected_service = st.selectbox(
                  f"Select Cloud Run Service (in {st.session_state.cloud_region}):",
                  options=st.session_state.available_services,
                  index=None,
                  key="service_name", # Binds to session state
                  placeholder="Choose a service..."
             )

        # Display Service Info (if service selected)
        if selected_project and selected_service:
             st.markdown("---")
             st.subheader(f"üìå Service Info: {selected_service}")
             with st.spinner("Loading service details..."):
                 service_info, service_urls, monitor_charts = helpers.load_service_data_from_cache(
                     cache_dir, selected_project, selected_service
                 )

                 if service_urls:
                      st.write("**URLs:**")
                      for url in service_urls:
                           st.code(url, language=None) # Display URLs clearly
                 else:
                     st.caption("No public URLs found in service.yaml")

                 if service_info:
                      with st.expander("Service Configuration (service.yaml)"):
                           st.text(service_info) # Or st.code(service_info, language='yaml') if you have pyyaml

                 else:
                      st.caption("service.yaml not found in cache.")

                 if monitor_charts:
                      st.write("**Monitoring Charts:**")
                      cols = st.columns(3) # Adjust grid as needed
                      col_idx = 0
                      for chart_path in monitor_charts:
                           with cols[col_idx % len(cols)]:
                                try:
                                    st.image(str(chart_path), caption=chart_path.name, use_column_width=True)
                                except Exception as e:
                                    st.error(f"Error loading {chart_path.name}: {e}")
                           col_idx += 1
                 else:
                     st.caption("No monitoring charts found in cache.")

             # Button to start investigation for this service
             if st.button(f"üïµÔ∏è Start Investigation for {selected_service}", use_container_width=True):
                 context = f"Project: {selected_project}, Service: {selected_service}, Region: {st.session_state.cloud_region}"
                 state.create_new_investigation(
                     name=f"Investigate {selected_service}",
                     context=context,
                     activate=True
                 )
                 # State change triggers rerun

        st.markdown("---")

        # --- Investigation Management ---
        st.header("üìú Investigations")

        if st.button("‚ûï New Investigation", use_container_width=True):
             state.create_new_investigation(activate=True)
             # state handles rerun

        # List existing investigations
        if st.session_state.investigations:
            sorted_investigations = sorted(
                st.session_state.investigations.items(),
                key=lambda item: item[1].get('created_at', '0'), # Sort by creation time
                reverse=True
            )
            for inv_id, inv_data in sorted_investigations:
                button_label = f"{inv_data.get('name', 'Untitled Investigation')}"
                button_type = "primary" if inv_id == st.session_state.current_investigation_id else "secondary"
                if st.button(button_label, key=f"inv_{inv_id}", type=button_type, use_container_width=True):
                    state.switch_investigation(inv_id)
                    # state handles rerun
        else:
            st.caption("No investigations yet. Start one!")


# --- Chat Interface Rendering ---
def render_chat_interface(db_file):
    """Renders the main chat area and handles user input."""

    if not st.session_state.current_investigation_id:
        st.info("üëà Select an investigation or start a new one from the sidebar.")
        return

    current_id = st.session_state.current_investigation_id
    investigation = st.session_state.investigations.get(current_id)

    if not investigation:
        st.error(f"üö® Could not load investigation {current_id}. Try selecting another.")
        st.session_state.current_investigation_id = None # Reset
        time.sleep(1) # Brief pause before rerun
        st.rerun()
        return

    st.subheader(f"üí¨ Chat: {investigation.get('name', 'Current Investigation')}")

    # Display chat messages
    for message in investigation["history"]:
        role = message.get("role")
        avatar = USER_AVATAR if role == "user" else MODEL_AVATAR
        with st.chat_message(role, avatar=avatar):
            # Display text content (handle potential structures)
            content = message.get("content", {}) # Assuming content is a dict
            text_parts = content.get("parts", []) if isinstance(content,dict) else [{"text": str(content)}] # Handle simple text too
            full_text = "\n".join([part.get("text", "") for part in text_parts if "text" in part])
            if full_text:
                st.markdown(full_text) # Render markdown

            # Display charts if available
            chart_filename = message.get("chart_filename")
            if chart_filename:
                chart_path = Path(chart_filename)
                if chart_path.is_file():
                    try:
                        st.image(str(chart_path), caption=f"üìä Chart: {chart_path.name}")
                    except Exception as e:
                        st.error(f"Failed to display chart {chart_path.name}: {e}")
                else:
                    st.warning(f"Chart file not found: {chart_filename}")

    # Chat input
    if prompt := st.chat_input("Ask Gemini about the Cloud Run service..."):
        # 1. Add user message to state and display it immediately
        state.add_message_to_current_investigation(role="user", text=prompt) # Persists here
        with st.chat_message("user", avatar=USER_AVATAR):
            st.markdown(prompt)

        # 2. Prepare history for Gemini
        # Adapt this based on how your ricc_genai expects history
        gemini_history = investigation["history"][:-1] # Send history *before* user prompt? Or include it? Check API.

        # 3. Call Gemini (handle streaming and function calling)
        try:
            with st.chat_message("assistant", avatar=MODEL_AVATAR):
                message_placeholder = st.empty()
                full_response_text = ""
                final_chart_filename = None

                # --- Streaming Call ---
                # Replace with your actual function call
                # This needs to handle function calls internally if possible, or return info about them
                # For now, simulate streaming:
                # response_stream = generate_content_with_function_calling(
                #     model_name=st.session_state.gemini_model,
                #     prompt=prompt,
                #     history=gemini_history,
                #     # ... other params like tools=...
                # )

                # Simulated stream with potential chart info at the end
                simulated_stream = ["TODO(ricc):", "Thinking... ü§î\n\n", "Okay, ", "let's look ", "into that. ", "Checking the ", "service status... ", "\n\nFound something! ", "Generating a chart... üìä"]
                #simulated_chart = "path/to/your/generated/chart.png" # Replace with actual logic result
                simulated_chart = ".cache/palladius-genai/cloud-run/gemini-news-crawler-dev/cloud-monitoring/experimental-charts-v3b/gemini-news-crawler-dev_traffic_latency_24h.png"

                # Use st.write_stream for real Gemini stream
                # full_response_text = message_placeholder.write_stream(response_stream)

                # Simulation logic:
                for chunk in simulated_stream:
                    full_response_text += chunk
                    message_placeholder.markdown(full_response_text + "‚ñå") # Simulate cursor
                    time.sleep(0.1)
                message_placeholder.markdown(full_response_text) # Final text without cursor
                final_chart_filename = simulated_chart # Set chart filename after text stream

                # --- Process Function Calls (if needed after streaming) ---
                # If your generate_content function returns function call info separately:
                # function_call = response.get("function_call") # Example structure
                # if function_call:
                #    tool_response = execute_function(function_call) # Your logic
                #    # Send tool_response back to Gemini in another call...
                #    # ... update message_placeholder with final Gemini answer
                #    pass # Placeholder

                # Display chart if generated
                if final_chart_filename:
                    chart_path = Path(final_chart_filename)
                    if chart_path.is_file():
                         try:
                              st.image(str(chart_path), caption=f"üìä Generated Chart: {chart_path.name}")
                         except Exception as e:
                              st.error(f"Failed display generated chart {chart_path.name}: {e}")
                    else:
                         st.warning(f"Generated chart file not found: {final_chart_filename}")


            # 4. Add final assistant response (including chart info) to state
            state.add_message_to_current_investigation(
                 role="assistant",
                 text=full_response_text, # Or use 'parts' if Gemini response uses it
                 chart_filename=final_chart_filename
            )
            # Note: Rerun is not needed here unless adding the message itself failed

        except Exception as e:
            st.error(f"ü§ñ Oops! Error calling Gemini: {e}")
            # Optionally add error message to chat history
            # state.add_message_to_current_investigation(role="assistant", text=f"Error: {e}")

